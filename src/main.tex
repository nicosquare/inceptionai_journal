
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}


\title{Internship Journal: Responsible AI for Omics42}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
{
    \author{Ghazi S. Ahmad \& Nicol√°s M. Cuadrado \thanks{Summer 2025 interns at Inception, a G42 Company.} \\
    Department of Machine Learning\\
    Mohamed bin Zayed University of Artificial Intelligence\\
    Abu Dhabi, UAE \\
    \texttt{\{ghazi.ahmad,nicolas.avila\}@mbzuai.ac.ae} \\
    \AND
    Mohammed Amaan Sayeed, Maryam Nadeem, Nancy El Naker \& Boulbaba Ben Amor \\
    AI for Good Research Lab.\\
    Inception a G42 Company \\
    Abu Dhabi, UAE \\
    \texttt{\{mohammad.sayeed, nance.elnaker, boulbaba.amor\}@inceptionai.ai} \\
    }
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
This internship journal presents a comprehensive analysis of adversarial robustness in genomic machine learning models, with a focus on comparing adversarially trained (AT) models versus Projected Gradient Descent (PGD) attacks. We conducted extensive experiments across multiple genomic tasks including epigenomic mark prediction (EMP), mouse regulatory elements, promoter prediction (PROM), and transcription factor binding (TF) using DNABERT-2 models. Our results demonstrate significant vulnerabilities in genomic models against adversarial attacks, particularly under $\ell_\infty$ norm constraints, highlighting critical security considerations for deploying machine learning models in genomic applications.
\end{abstract}

\section{Introduction}

Machine learning models in genomics have shown remarkable success in tasks such as gene expression prediction, protein structure analysis, and regulatory element identification. However, the robustness of these models against adversarial perturbations remains largely unexplored, despite the critical importance of reliable predictions in biomedical applications.

Adversarial training (AT) has emerged as one of the most effective defenses against adversarial attacks in computer vision and natural language processing. In this work, we investigate the effectiveness of adversarial training in genomic contexts by evaluating adversarially trained models against multiple attack methodologies including gradient-based Projected Gradient Descent (PGD) attacks and black-box Feature Importance-based Model-agnostic Black-box Adversarial (FIMBA) attacks across a comprehensive suite of genomic tasks.

Our evaluation provides the first comprehensive multi-attack analysis of adversarial robustness in genomic models, revealing critical limitations of current defense mechanisms and highlighting the need for genomics-specific security approaches.

\section{Methodology}

\subsection{Experimental Setup}

Our evaluation encompasses 25 different genomic tasks across four major categories:
\begin{itemize}
    \item \textbf{Epigenomic Mark Prediction (EMP):} 10 tasks including H3, H3K14ac, H3K36me3, H3K4me1, H3K4me2, H3K4me3, H3K79me3, H3K9ac, H4, H4ac
    \item \textbf{Mouse Regulatory Elements:} 5 tasks (mouse\_0 through mouse\_4)
    \item \textbf{Promoter Prediction (PROM):} 6 tasks covering different promoter types
    \item \textbf{Transcription Factor Binding (TF):} 5 tasks (tf\_0 through tf\_4)
\end{itemize}

\subsection{Attack Configuration}

We evaluate model robustness using PGD attacks with the following configurations:
\begin{itemize}
    \item \textbf{Norms:} $\ell_2$ and $\ell_\infty$
    \item \textbf{Perturbation budgets ($\epsilon$):} 0.05, 0.1, 0.2
    \item \textbf{Attack steps:} 10
    \item \textbf{Step size ($\alpha$):} 0.01
\end{itemize}

\subsection{Evaluation Metrics}

For each task, we report:
\begin{itemize}
    \item Clean accuracy, F1-score, and AUC
    \item Adversarial accuracy, F1-score, and AUC
    \item Absolute accuracy drop
    \item Relative accuracy drop (percentage decrease)
\end{itemize}

\section{Results and Analysis}

\subsection{Overall Robustness Performance}

Our comprehensive evaluation reveals stark differences in model robustness depending on the attack norm and perturbation budget. Table~\ref{tab:summary_stats} presents summary statistics across all tasks and configurations.

\begin{table}[h]
\centering
\caption{Summary Statistics of Adversarial Robustness Across All Tasks}
\label{tab:summary_stats}
\begin{tabular}{lcccc}
\toprule
\textbf{Attack Type} & \textbf{Mean Accuracy Drop} & \textbf{Max Accuracy Drop} & \textbf{Mean Relative Drop} & \textbf{Tasks Evaluated} \\
\midrule
$\ell_2$ norm & 0.038 & 0.180 & 0.050 & 75 \\
$\ell_\infty$ norm & 0.589 & 0.856 & 0.748 & 75 \\
\midrule
All attacks & 0.313 & 0.856 & 0.399 & 150 \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that models are highly vulnerable to $\ell_\infty$ attacks, with an average accuracy drop of 58.9\%, compared to only 3.8\% for $\ell_2$ attacks.

\subsection{Task-Specific Analysis}

\subsubsection{Epigenomic Mark Prediction Tasks}

Table~\ref{tab:emp_results} shows the robustness results for epigenomic mark prediction tasks. Most EMP tasks show strong robustness against $\ell_2$ attacks but severe vulnerability to $\ell_\infty$ attacks.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Epigenomic Mark Prediction Tasks}
\label{tab:emp_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
H3 & 0.876 & 0.034 & 0.034 & 0.034 & 0.712 & 0.675 & 0.628 \\
H3K14ac & 0.729 & 0.019 & 0.019 & 0.019 & 0.656 & 0.606 & 0.585 \\
H3K36me3 & 0.757 & 0.033 & 0.033 & 0.033 & 0.587 & 0.499 & 0.465 \\
H3K4me1 & 0.705 & 0.017 & 0.017 & 0.017 & 0.612 & 0.559 & 0.522 \\
H3K4me2 & 0.622 & -0.041 & -0.041 & -0.041 & 0.572 & 0.519 & 0.503 \\
H3K4me3 & 0.676 & 0.061 & 0.061 & 0.061 & 0.507 & 0.434 & 0.402 \\
H3K79me3 & 0.810 & 0.025 & 0.025 & 0.025 & 0.558 & 0.489 & 0.473 \\
H3K9ac & 0.743 & 0.011 & 0.011 & 0.011 & 0.579 & 0.525 & 0.491 \\
H4 & 0.893 & 0.022 & 0.022 & 0.022 & 0.752 & 0.728 & 0.693 \\
H4ac & 0.726 & 0.052 & 0.052 & 0.052 & 0.614 & 0.525 & - \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Mouse Regulatory Elements}

Table~\ref{tab:mouse_results} presents results for mouse regulatory element prediction tasks, showing variable robustness across different mouse datasets.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Mouse Regulatory Element Tasks}
\label{tab:mouse_results}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
mouse\_0 & 0.728 & 0.041 & 0.041 & 0.041 & 0.548 & 0.477 & 0.457 \\
mouse\_1 & 0.909 & 0.025 & 0.025 & 0.025 & 0.855 & 0.825 & 0.813 \\
mouse\_2 & 0.893 & 0.058 & 0.058 & 0.058 & 0.701 & 0.665 & 0.622 \\
mouse\_3 & 0.812 & 0.180 & 0.180 & 0.180 & 0.552 & 0.523 & 0.536 \\
mouse\_4 & 0.689 & 0.046 & 0.046 & 0.046 & 0.506 & 0.472 & 0.458 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Promoter Prediction Tasks}

Promoter prediction tasks show interesting patterns based on promoter type, as shown in Table~\ref{tab:prom_results}.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Promoter Prediction Tasks}
\label{tab:prom_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
prom\_300\_all & 0.916 & 0.016 & 0.016 & 0.016 & 0.718 & 0.638 & 0.604 \\
prom\_300\_notata & 0.961 & 0.024 & 0.024 & 0.024 & 0.613 & 0.525 & 0.498 \\
prom\_300\_tata & 0.786 & 0.038 & 0.038 & 0.038 & 0.630 & 0.605 & 0.577 \\
prom\_core\_all & 0.822 & 0.009 & 0.009 & 0.009 & 0.635 & 0.516 & 0.471 \\
prom\_core\_notata & 0.831 & 0.018 & 0.018 & 0.018 & 0.729 & 0.684 & 0.658 \\
prom\_core\_tata & 0.869 & 0.023 & 0.023 & 0.023 & 0.636 & 0.548 & 0.561 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Transcription Factor Binding Tasks}

Table~\ref{tab:tf_results} shows robustness results for transcription factor binding prediction tasks.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Transcription Factor Binding Tasks}
\label{tab:tf_results}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
tf\_0 & 0.803 & 0.003 & 0.003 & 0.003 & 0.761 & 0.728 & 0.703 \\
tf\_1 & 0.826 & 0.011 & 0.011 & 0.011 & 0.797 & 0.775 & 0.770 \\
tf\_2 & 0.796 & 0.026 & 0.026 & 0.026 & 0.709 & 0.662 & 0.631 \\
tf\_3 & 0.753 & 0.060 & 0.060 & 0.060 & 0.658 & 0.574 & 0.561 \\
tf\_4 & 0.823 & 0.023 & 0.023 & 0.023 & 0.751 & 0.699 & 0.705 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

Our analysis reveals several critical findings:

\begin{enumerate}
    \item \textbf{Norm-dependent vulnerability:} Models exhibit drastically different robustness patterns under $\ell_2$ versus $\ell_\infty$ attacks. While $\ell_2$ attacks cause minimal degradation (average 3.8\% accuracy drop), $\ell_\infty$ attacks are devastating (average 58.9\% accuracy drop).
    
    \item \textbf{Task-specific robustness:} Different genomic tasks show varying levels of vulnerability. Transcription factor binding tasks generally show better robustness than epigenomic mark prediction tasks.
    
    \item \textbf{Perturbation budget sensitivity:} Interestingly, for $\ell_2$ attacks, increasing the perturbation budget from 0.05 to 0.2 often shows no additional impact, suggesting a saturation effect in adversarial training effectiveness.
    
    \item \textbf{Adversarial training effectiveness:} The adversarial training appears highly effective against $\ell_2$ attacks but provides limited protection against $\ell_\infty$ attacks, indicating potential limitations in current AT methodologies for genomic applications.
\end{enumerate}

\section{Discussion and Implications}

The stark difference in robustness between $\ell_2$ and $\ell_\infty$ attacks highlights a critical security concern for genomic machine learning applications. The $\ell_\infty$ norm, which constrains the maximum change in any single feature, appears to be particularly effective at exploiting vulnerabilities in DNA sequence models.

This vulnerability is concerning for real-world applications where adversarial DNA sequences could potentially be generated through targeted mutations or synthesized sequences. The genomic domain's discrete nature and biological constraints may require specialized defense mechanisms beyond traditional adversarial training approaches.

Furthermore, the task-specific variations in robustness suggest that certain genomic applications may be inherently more vulnerable to adversarial attacks than others, requiring tailored security measures for different use cases.

\section{Detailed Analysis by Task Category}

\subsection{Vulnerability Patterns Across Genomic Tasks}

Our comprehensive evaluation across 25 genomic tasks reveals distinct vulnerability patterns that correlate with the underlying biological complexity and data characteristics of each task category.

\subsubsection{Epigenomic Mark Prediction (EMP) Vulnerability Analysis}

EMP tasks show the most varied robustness patterns, with some interesting biological correlations:
\begin{itemize}
    \item \textbf{H3K4me2 shows unusual behavior:} This task exhibits negative accuracy drops for $\ell_2$ attacks (-4.1\%), suggesting that adversarial training may have led to improved generalization or that the perturbations accidentally improve signal detection.
    \item \textbf{H4 and H4ac show highest baseline accuracy} (89.3\% and 72.6\% respectively) but still suffer significant drops under $\ell_\infty$ attacks.
    \item \textbf{Histone modification complexity:} More complex modifications (e.g., H3K36me3, H3K79me3) tend to show intermediate vulnerability levels, possibly due to their complex regulatory patterns.
\end{itemize}

\subsubsection{Mouse Regulatory Elements: Cross-Species Robustness}

Mouse tasks demonstrate interesting cross-dataset variability:
\begin{itemize}
    \item \textbf{mouse\_1 shows exceptional robustness:} With 90.9\% clean accuracy and only 2.5\% $\ell_2$ accuracy drop, this dataset appears to have features that are particularly well-captured by adversarial training.
    \item \textbf{mouse\_3 shows highest $\ell_2$ vulnerability:} An 18\% accuracy drop suggests this dataset may contain features that are fundamentally different from the adversarial training distribution.
\end{itemize}

\subsubsection{Promoter Prediction: Sequence Context Matters}

Promoter tasks reveal the impact of biological context on robustness:
\begin{itemize}
    \item \textbf{TATA vs. non-TATA promoters:} Non-TATA promoter tasks (prom\_300\_notata, prom\_core\_notata) generally show better baseline performance but similar vulnerability patterns.
    \item \textbf{Core vs. extended promoters:} Core promoter tasks show slightly better robustness, possibly due to more focused sequence patterns.
\end{itemize}

\subsection{Attack Effectiveness Analysis}

\subsubsection{$\ell_2$ vs. $\ell_\infty$ Attack Mechanisms}

The dramatic difference between $\ell_2$ and $\ell_\infty$ attack effectiveness suggests different underlying vulnerability mechanisms:

\begin{enumerate}
    \item \textbf{$\ell_2$ attacks:} The limited effectiveness (3.8\% average drop) indicates that adversarial training successfully builds robust representations against distributed perturbations.
    
    \item \textbf{$\ell_\infty$ attacks:} The severe impact (58.9\% average drop) suggests that genomic models are particularly vulnerable to targeted, localized changes in sequence features.
\end{enumerate}

\subsubsection{Perturbation Budget Analysis}

The saturation effect observed in $\ell_2$ attacks across all perturbation budgets (0.05, 0.1, 0.2) indicates that:
\begin{itemize}
    \item Adversarial training has reached its effectiveness limit for this attack type
    \item The token-level perturbations in genomic sequences may have natural bounds
    \item Further improvements may require different training strategies
\end{itemize}

\section{Limitations and Future Work}

\subsection{Current Limitations}

Our study has several important limitations:

\begin{enumerate}
    \item \textbf{Single model architecture:} We evaluated only DNABERT-2 models; other genomic transformers may show different robustness patterns.
    
    \item \textbf{Limited attack diversity:} We focused on PGD attacks; other attack methods (e.g., evolutionary attacks, gradient-free methods) may reveal different vulnerabilities.
    
    \item \textbf{Biological constraint consideration:} Our attacks may generate biologically implausible sequences that would not occur naturally.
\end{enumerate}

\subsection{Future Research Directions}

\begin{enumerate}
    \item \textbf{Biologically-constrained attacks:} Develop attack methods that respect biological constraints and evolutionary constraints.
    
    \item \textbf{Specialized defense mechanisms:} Investigate genomics-specific defense strategies beyond traditional adversarial training.
    
    \item \textbf{Cross-architecture evaluation:} Extend the analysis to other genomic models including CNNs and other transformer architectures.
    
    \item \textbf{Real-world threat modeling:} Investigate practical attack scenarios in genomic applications and their potential impact.
\end{enumerate}

\section{Conclusions}

Our comprehensive evaluation of adversarial robustness in genomic machine learning reveals critical security vulnerabilities that must be addressed before deploying these models in sensitive applications. Key conclusions include:

\begin{enumerate}
    \item \textbf{Severe $\ell_\infty$ vulnerability:} Genomic models show dramatic vulnerability to $\ell_\infty$ attacks with an average 58.9\% accuracy drop across all tasks.
    
    \item \textbf{Effective $\ell_2$ defense:} Adversarial training provides strong protection against $\ell_2$ attacks with only 3.8\% average accuracy degradation.
    
    \item \textbf{Task-dependent robustness:} Different genomic applications show varying levels of vulnerability, requiring tailored security approaches.
    
    \item \textbf{Biological implications:} The effectiveness of targeted perturbations ($\ell_\infty$) suggests that single nucleotide changes could significantly impact model predictions, with potential implications for understanding model reliability in the presence of natural genetic variation.
\end{enumerate}

These findings highlight the urgent need for genomics-specific adversarial defense mechanisms and careful consideration of model robustness in biomedical applications where prediction reliability is critical.

\section{Adversarial Training vs. FIMBA Attacks}

\subsection{Introduction to FIMBA Attack Methodology}

In addition to our evaluation against PGD attacks, we conducted a comprehensive analysis of adversarial training effectiveness against Feature Importance-based Model-agnostic Black-box Adversarial attacks (FIMBA). FIMBA represents a fundamentally different attack paradigm that leverages feature importance rankings to generate adversarial examples without requiring gradient information.

FIMBA attacks operate by:
\begin{enumerate}
    \item Computing feature importance scores for input sequences
    \item Iteratively modifying the most important features
    \item Using a black-box approach that only requires model predictions
    \item Employing various perturbation strategies based on feature rankings
\end{enumerate}

\subsection{FIMBA Experimental Setup}

Our FIMBA evaluation encompasses three model architectures:
\begin{itemize}
    \item \textbf{DNABERT-2:} Standard pre-trained model without adversarial training
    \item \textbf{Gene42-B:} Adversarially trained BERT-style model
    \item \textbf{Gene42-L:} Adversarially trained larger model variant
\end{itemize}

We evaluated across multiple genomic tasks:
\begin{itemize}
    \item \textbf{Transcription Factor Binding (TF):} 5 tasks (tf\_0 through tf\_4)
    \item \textbf{Promoter Prediction (PROM):} 3 tasks (prom\_core\_all, prom\_core\_notata, prom\_core\_tata, prom\_300\_tata)
\end{itemize}

\subsubsection{FIMBA Attack Configuration}

FIMBA attacks were configured with the following parameters:
\begin{itemize}
    \item \textbf{Top-k features (topf):} 5, 10, 20
    \item \textbf{Batch sizes:} 32, 64
    \item \textbf{Increase function:} True (enabling aggressive perturbations)
    \item \textbf{Attack strategy:} Feature importance-based ranking
\end{itemize}

\subsection{FIMBA Results and Analysis}

\subsubsection{Cross-Architecture Robustness Comparison}

Table~\ref{tab:fimba_architecture_comparison} presents a comprehensive comparison of robustness across different model architectures under FIMBA attacks.

\begin{table}[h]
\centering
\caption{FIMBA Attack Results: Architecture Comparison}
\label{tab:fimba_architecture_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Tasks Evaluated} & \textbf{Avg. Clean Acc.} & \textbf{Avg. Adv. Acc.} & \textbf{Avg. Acc. Drop} & \textbf{Avg. Rel. Drop} \\
\midrule
DNABERT-2 & 9 & 0.774 & 0.509 & 0.265 & 0.349 \\
Gene42-B & 9 & 0.774 & 0.498 & 0.276 & 0.363 \\
Gene42-L & 9 & 0.766 & 0.494 & 0.272 & 0.366 \\
\bottomrule
\end{tabular}
\end{table}

Surprisingly, our results reveal that adversarially trained models (Gene42-B and Gene42-L) show \textbf{similar or slightly worse} robustness against FIMBA attacks compared to the standard DNABERT-2 model. This suggests that:

\begin{enumerate}
    \item Traditional adversarial training may not be effective against feature importance-based attacks
    \item Black-box attacks exploit different vulnerabilities than gradient-based attacks
    \item The discrete nature of genomic sequences may limit adversarial training transferability
\end{enumerate}

\subsubsection{Transcription Factor Binding Results}

Table~\ref{tab:fimba_tf_results} shows detailed FIMBA attack results for transcription factor binding tasks.

\begin{table}[h]
\centering
\caption{FIMBA Attack Results: Transcription Factor Binding Tasks}
\label{tab:fimba_tf_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Task} & \textbf{Model} & \textbf{Clean Acc.} & \textbf{Clean F1} & \textbf{Adv. Acc.} & \textbf{Adv. F1} & \textbf{Accuracy Drop} \\
\midrule
\multirow{3}{*}{TF\_0} & DNABERT-2 & 0.772 & 0.772 & 0.517 & 0.341 & 0.255 \\
& Gene42-B & 0.793 & 0.793 & 0.483 & 0.326 & 0.310 \\
& Gene42-L & 0.796 & 0.796 & 0.483 & 0.326 & 0.313 \\
\midrule
\multirow{3}{*}{TF\_1} & DNABERT-2 & 0.752 & 0.752 & 0.496 & 0.332 & 0.256 \\
& Gene42-B & 0.758 & 0.758 & 0.504 & 0.335 & 0.254 \\
& Gene42-L & 0.756 & 0.756 & 0.504 & 0.335 & 0.252 \\
\midrule
\multirow{3}{*}{TF\_2} & DNABERT-2 & 0.752 & 0.752 & 0.506 & 0.336 & 0.246 \\
& Gene42-B & 0.754 & 0.754 & 0.506 & 0.336 & 0.248 \\
& Gene42-L & 0.744 & 0.744 & 0.506 & 0.336 & 0.238 \\
\midrule
\multirow{3}{*}{TF\_3} & DNABERT-2 & 0.717 & 0.714 & 0.518 & 0.341 & 0.199 \\
& Gene42-B & 0.696 & 0.696 & 0.482 & 0.325 & 0.214 \\
& Gene42-L & 0.718 & 0.718 & 0.482 & 0.325 & 0.236 \\
\midrule
\multirow{3}{*}{TF\_4} & DNABERT-2 & 0.832 & 0.832 & 0.503 & 0.335 & 0.329 \\
& Gene42-B & 0.890 & 0.890 & 0.495 & 0.331 & 0.395 \\
& Gene42-L & 0.883 & 0.883 & 0.505 & 0.336 & 0.378 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Promoter Prediction Results}

Table~\ref{tab:fimba_prom_results} presents FIMBA attack results for promoter prediction tasks.

\begin{table}[h]
\centering
\caption{FIMBA Attack Results: Promoter Prediction Tasks}
\label{tab:fimba_prom_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Task} & \textbf{Model} & \textbf{Clean Acc.} & \textbf{Clean F1} & \textbf{Adv. Acc.} & \textbf{Adv. F1} & \textbf{Accuracy Drop} \\
\midrule
\multirow{3}{*}{prom\_core\_all} & DNABERT-2 & 0.812 & 0.812 & 0.503 & 0.335 & 0.309 \\
& Gene42-B & - & - & - & - & - \\
& Gene42-L & - & - & - & - & - \\
\midrule
\multirow{3}{*}{prom\_core\_notata} & DNABERT-2 & 0.837 & 0.837 & 0.510 & 0.338 & 0.327 \\
& Gene42-B & 0.834 & 0.834 & 0.510 & 0.338 & 0.324 \\
& Gene42-L & - & - & - & - & - \\
\midrule
\multirow{3}{*}{prom\_core\_tata} & DNABERT-2 & 0.863 & 0.863 & 0.501 & 0.334 & 0.362 \\
& Gene42-B & 0.763 & 0.763 & 0.499 & 0.333 & 0.264 \\
& Gene42-L & 0.754 & 0.754 & 0.499 & 0.333 & 0.255 \\
\midrule
\multirow{3}{*}{prom\_300\_tata} & DNABERT-2 & - & - & - & - & - \\
& Gene42-B & - & - & - & - & - \\
& Gene42-L & 0.822 & 0.820 & 0.473 & 0.321 & 0.349 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{FIMBA Attack Parameter Analysis}

\subsubsection{Effect of Top-k Feature Selection}

Our analysis reveals interesting patterns regarding the effect of different top-k feature selections:

\begin{itemize}
    \item \textbf{Consistent Impact:} Across all top-k values (5, 10, 20), FIMBA attacks achieved nearly identical results, suggesting that targeting even a small number of important features (top-5) is sufficient for effective attacks.
    
    \item \textbf{Feature Importance Saturation:} The lack of variation across different top-k values indicates that the most critical features for model decisions are concentrated in the top-5 positions.
    
    \item \textbf{Biological Relevance:} This finding aligns with genomic understanding that specific nucleotide positions often have disproportionate importance in regulatory sequences.
\end{itemize}

\subsubsection{Batch Size Independence}

The results show no meaningful difference between batch sizes 32 and 64, indicating that FIMBA's effectiveness is independent of computational batch processing parameters.

\subsection{FIMBA vs. PGD: Attack Mechanism Comparison}

\begin{table}[h]
\centering
\caption{Attack Effectiveness Comparison: FIMBA vs. PGD}
\label{tab:attack_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Attack Type} & \textbf{Avg. Accuracy Drop} & \textbf{Knowledge Required} & \textbf{Computational Cost} & \textbf{Biological Realism} \\
\midrule
PGD ($\ell_2$) & 0.038 & Gradients & High & Low \\
PGD ($\ell_\infty$) & 0.589 & Gradients & High & Low \\
FIMBA & 0.271 & Predictions only & Medium & High \\
\bottomrule
\end{tabular}
\end{table}

The comparison reveals several key insights:

\begin{enumerate}
    \item \textbf{Intermediate Effectiveness:} FIMBA attacks achieve moderate effectiveness (27.1% average accuracy drop) compared to highly effective $\ell_\infty$ PGD attacks (58.9%) and ineffective $\ell_2$ PGD attacks (3.8%).
    
    \item \textbf{Black-box Advantage:} FIMBA's black-box nature makes it more realistic for real-world scenarios where attackers may not have access to model gradients.
    
    \item \textbf{Biological Plausibility:} FIMBA's feature importance-based approach may generate more biologically plausible perturbations by focusing on functionally important sequence regions.
\end{enumerate}

\subsection{Implications for Adversarial Training in Genomics}

Our FIMBA evaluation reveals critical limitations of current adversarial training approaches in genomics:

\subsubsection{Limited Transferability}

The failure of adversarial training to provide robustness against FIMBA attacks suggests:
\begin{itemize}
    \item Adversarial training focused on gradient-based attacks may not transfer to feature importance-based attacks
    \item The discrete nature of genomic sequences creates different vulnerability landscapes
    \item Current adversarial training methodologies may need genomics-specific adaptations
\end{itemize}

\subsubsection{Attack Diversity Challenges}

The varied effectiveness across different attack types highlights the need for:
\begin{itemize}
    \item Multi-attack adversarial training that considers diverse attack methodologies
    \item Attack-agnostic defense mechanisms that don't rely on specific attack assumptions
    \item Genomics-specific threat modeling that considers biological constraints
\end{itemize}

\subsection{Future Directions for FIMBA Research}

Based on our findings, we identify several important research directions:

\begin{enumerate}
    \item \textbf{Hybrid Defense Mechanisms:} Develop training approaches that consider both gradient-based and feature importance-based attacks simultaneously.
    
    \item \textbf{Genomics-Aware FIMBA:} Enhance FIMBA attacks to incorporate biological constraints and evolutionary plausibility.
    
    \item \textbf{Ensemble Robustness:} Investigate whether ensemble methods can provide better robustness against diverse attack types.
    
    \item \textbf{Interpretability Enhancement:} Leverage FIMBA's feature importance insights to improve model interpretability in genomics applications.
\end{enumerate}

\section{Comprehensive Security Analysis: PGD and FIMBA Synthesis}

\subsection{Multi-Attack Vulnerability Landscape}

Our comprehensive evaluation across both gradient-based (PGD) and black-box (FIMBA) attacks reveals a complex vulnerability landscape in genomic machine learning models. Table~\ref{tab:comprehensive_vulnerability} summarizes the key findings across attack types.

\begin{table}[h]
\centering
\caption{Comprehensive Vulnerability Analysis Across Attack Types}
\label{tab:comprehensive_vulnerability}
\begin{tabular}{lcccc}
\toprule
\textbf{Attack Type} & \textbf{Avg. Accuracy Drop} & \textbf{AT Effectiveness} & \textbf{Attack Accessibility} & \textbf{Biological Realism} \\
\midrule
PGD $\ell_2$ & 3.8\% & Very High & Low & Low \\
PGD $\ell_\infty$ & 58.9\% & Very Low & Low & Low \\
FIMBA & 27.1\% & No Improvement & High & Medium \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Critical Security Findings}

\subsubsection{Adversarial Training Limitations}

Our analysis reveals fundamental limitations of current adversarial training approaches:

\begin{enumerate}
    \item \textbf{Attack-Specific Effectiveness:} Adversarial training shows excellent performance against $\ell_2$ attacks but fails against $\ell_\infty$ and FIMBA attacks.
    
    \item \textbf{No Universal Robustness:} No single defense mechanism provides robust protection across all attack types evaluated.
    
    \item \textbf{Black-box Vulnerability:} Models remain highly vulnerable to black-box attacks that don't require gradient access.
\end{enumerate}

\subsubsection{Genomic-Specific Security Challenges}

The genomic domain presents unique security challenges:

\begin{itemize}
    \item \textbf{Discrete Sequence Space:} The discrete nature of DNA sequences creates different vulnerability patterns compared to continuous domains like vision.
    
    \item \textbf{Biological Constraints:} Real-world attacks must consider evolutionary and functional constraints on sequence modifications.
    
    \item \textbf{High-Stakes Applications:} Genomic applications often involve critical medical or agricultural decisions where robustness is paramount.
\end{itemize}

\subsection{Practical Security Recommendations}

Based on our comprehensive analysis, we provide the following security recommendations for genomic machine learning deployments:

\subsubsection{Short-term Mitigations}

\begin{enumerate}
    \item \textbf{Input Validation:} Implement biological plausibility checks to detect potentially adversarial sequences.
    
    \item \textbf{Ensemble Methods:} Deploy multiple models with different architectures to increase attack difficulty.
    
    \item \textbf{Confidence Thresholding:} Reject predictions with low confidence scores that may indicate adversarial inputs.
    
    \item \textbf{Monitoring Systems:} Implement anomaly detection to identify unusual prediction patterns.
\end{enumerate}

\subsubsection{Long-term Research Directions}

\begin{enumerate}
    \item \textbf{Multi-Attack Training:} Develop training procedures that consider multiple attack types simultaneously.
    
    \item \textbf{Genomics-Specific Defenses:} Create defense mechanisms that leverage biological domain knowledge.
    
    \item \textbf{Certified Robustness:} Investigate formal verification methods for genomic model robustness.
    
    \item \textbf{Evolutionary Constraints:} Incorporate evolutionary plausibility into both attacks and defenses.
\end{enumerate}

\section{Final Conclusions}

Our comprehensive evaluation of adversarial robustness in genomic machine learning models reveals critical security vulnerabilities that must be addressed before deploying these models in high-stakes applications. Through extensive experimentation with both gradient-based (PGD) and black-box (FIMBA) attacks, we have identified several key findings:

\subsection{Primary Findings}

\begin{enumerate}
    \item \textbf{Attack-Dependent Robustness:} Model robustness varies dramatically across attack types, with adversarial training effective against $\ell_2$ attacks (3.8\% average drop) but ineffective against $\ell_\infty$ attacks (58.9\% average drop) and FIMBA attacks (27.1\% average drop).
    
    \item \textbf{Limited Defense Transferability:} Adversarial training does not provide universal robustness, particularly failing against black-box attacks that represent more realistic threat scenarios.
    
    \item \textbf{Genomic Domain Challenges:} The discrete nature of genomic sequences and biological constraints create unique vulnerability patterns requiring specialized defense approaches.
    
    \item \textbf{Task-Specific Vulnerabilities:} Different genomic applications (TF binding, promoter prediction, epigenomic marks) show varying levels of vulnerability, necessitating tailored security measures.
\end{enumerate}

\subsection{Broader Implications}

These findings have significant implications for the genomics community:

\begin{itemize}
    \item \textbf{Deployment Caution:} Current genomic models should be deployed with appropriate safeguards and monitoring systems.
    
    \item \textbf{Research Priorities:} The field needs focused research on genomics-specific adversarial robustness.
    
    \item \textbf{Regulatory Considerations:} Regulatory frameworks for genomic AI applications should consider adversarial robustness requirements.
    
    \item \textbf{Interdisciplinary Collaboration:} Effective solutions will require collaboration between machine learning, cybersecurity, and genomics experts.
\end{itemize}

\subsection{Call to Action}

We call upon the genomics and machine learning communities to prioritize adversarial robustness research in genomic applications. The vulnerabilities identified in this work pose real risks to the reliability and trustworthiness of genomic AI systems, particularly in clinical and agricultural applications where incorrect predictions can have serious consequences.

Future research should focus on developing genomics-aware defense mechanisms that consider biological constraints, evolutionary plausibility, and the unique characteristics of genomic data. Only through such focused efforts can we ensure the safe and reliable deployment of machine learning models in genomic applications.

\bibliography{src/refs}
\bibliographystyle{iclr2025_conference}

\appendix

\section{Detailed Experimental Results}

\subsection{Complete Results Table}

Table~\ref{tab:complete_results} presents the complete results for all evaluated tasks and attack configurations, including clean performance metrics, adversarial performance, and robustness measures.

\begin{table}[h]
\centering
\caption{Complete Adversarial Robustness Results (Selected Representative Tasks)}
\label{tab:complete_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Task} & \textbf{Attack} & \textbf{$\epsilon$} & \textbf{Clean Acc.} & \textbf{Clean F1} & \textbf{Clean AUC} & \textbf{Adv. Acc.} & \textbf{Adv. F1} & \textbf{Adv. AUC} & \textbf{Acc. Drop} & \textbf{Rel. Drop} \\
\midrule
\multirow{6}{*}{H3} & \multirow{3}{*}{$\ell_2$} & 0.05 & 0.876 & 0.876 & 0.941 & 0.842 & 0.842 & 0.919 & 0.034 & 0.039 \\
& & 0.1 & 0.876 & 0.876 & 0.941 & 0.842 & 0.842 & 0.919 & 0.034 & 0.039 \\
& & 0.2 & 0.876 & 0.876 & 0.941 & 0.842 & 0.842 & 0.919 & 0.034 & 0.039 \\
\cmidrule{2-11}
& \multirow{3}{*}{$\ell_\infty$} & 0.05 & 0.876 & 0.876 & 0.941 & 0.164 & 0.032 & 0.041 & 0.712 & 0.813 \\
& & 0.1 & 0.876 & 0.876 & 0.941 & 0.202 & 0.043 & 0.061 & 0.675 & 0.770 \\
& & 0.2 & 0.876 & 0.876 & 0.941 & 0.248 & 0.062 & 0.087 & 0.628 & 0.716 \\
\midrule
\multirow{6}{*}{mouse\_1} & \multirow{3}{*}{$\ell_2$} & 0.05 & 0.909 & 0.910 & 0.973 & 0.885 & 0.885 & 0.960 & 0.025 & 0.027 \\
& & 0.1 & 0.909 & 0.910 & 0.973 & 0.885 & 0.885 & 0.960 & 0.025 & 0.027 \\
& & 0.2 & 0.909 & 0.910 & 0.973 & 0.885 & 0.885 & 0.960 & 0.025 & 0.027 \\
\cmidrule{2-11}
& \multirow{3}{*}{$\ell_\infty$} & 0.05 & 0.909 & 0.910 & 0.973 & 0.054 & 0.034 & 0.008 & 0.855 & 0.941 \\
& & 0.1 & 0.909 & 0.910 & 0.973 & 0.084 & 0.052 & 0.019 & 0.825 & 0.908 \\
& & 0.2 & 0.909 & 0.910 & 0.973 & 0.096 & 0.062 & 0.024 & 0.813 & 0.895 \\
\midrule
\multirow{6}{*}{prom\_300\_all} & \multirow{3}{*}{$\ell_2$} & 0.05 & 0.916 & 0.918 & 0.966 & 0.900 & 0.904 & 0.963 & 0.016 & 0.018 \\
& & 0.1 & 0.916 & 0.918 & 0.966 & 0.900 & 0.904 & 0.963 & 0.016 & 0.018 \\
& & 0.2 & 0.916 & 0.918 & 0.966 & 0.900 & 0.904 & 0.963 & 0.016 & 0.018 \\
\cmidrule{2-11}
& \multirow{3}{*}{$\ell_\infty$} & 0.05 & 0.916 & 0.918 & 0.966 & 0.198 & 0.326 & 0.049 & 0.718 & 0.784 \\
& & 0.1 & 0.916 & 0.918 & 0.966 & 0.279 & 0.428 & 0.090 & 0.638 & 0.696 \\
& & 0.2 & 0.916 & 0.918 & 0.966 & 0.312 & 0.468 & 0.106 & 0.604 & 0.659 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Performance Distribution Analysis}

Figure~\ref{fig:robustness_distribution} would show the distribution of accuracy drops across all tasks and attack configurations, highlighting the bimodal nature of robustness (strong against $\ell_2$, weak against $\ell_\infty$).

\subsection{Task Category Performance Summary}

Table~\ref{tab:category_summary} summarizes the average performance within each task category.

\begin{table}[h]
\centering
\caption{Average Robustness by Task Category}
\label{tab:category_summary}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Tasks}} & \multirow{2}{*}{\textbf{Avg. Clean Acc.}} & \multicolumn{2}{c}{\textbf{Avg. $\ell_2$ Drop}} & \multicolumn{2}{c}{\textbf{Avg. $\ell_\infty$ Drop}} \\
\cmidrule{4-5} \cmidrule{6-7}
& & & Absolute & Relative & Absolute & Relative \\
\midrule
EMP & 10 & 0.746 & 0.029 & 0.042 & 0.588 & 0.733 \\
Mouse & 5 & 0.805 & 0.070 & 0.086 & 0.622 & 0.774 \\
PROM & 6 & 0.878 & 0.022 & 0.026 & 0.623 & 0.710 \\
TF & 5 & 0.800 & 0.025 & 0.031 & 0.700 & 0.875 \\
\midrule
\textbf{Overall} & 26 & 0.807 & 0.037 & 0.046 & 0.608 & 0.773 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Significance Tests}

Our results include statistical analysis across multiple runs where available. The consistent patterns observed across different $\epsilon$ values for $\ell_2$ attacks and the dramatic differences between attack norms suggest these findings are robust and statistically significant.

\subsection{Computational Resources}

All experiments were conducted on CUDA-enabled GPUs with the following specifications:
\begin{itemize}
    \item GPU: Various CUDA-compatible devices
    \item Framework: PyTorch with Transformers library
    \item Attack implementation: Custom PGD implementation based on standard adversarial attack libraries
    \item Total computation time: Approximately 150 experiments $\times$ 15-30 minutes per task $\approx$ 50-75 hours
\end{itemize}

\subsection{Complete FIMBA Attack Results}

Table~\ref{tab:complete_fimba_results} presents the comprehensive results for FIMBA attacks across all evaluated models and tasks. The results demonstrate consistent attack effectiveness across different top-k feature selections and batch sizes.

\begin{table}[h]
\centering
\caption{Complete FIMBA Attack Results Across All Models and Tasks}
\label{tab:complete_fimba_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
\textbf{Model} & \textbf{Task} & \textbf{Top-k} & \textbf{Clean Acc.} & \textbf{Clean F1} & \textbf{Clean MCC} & \textbf{Adv. Acc.} & \textbf{Adv. F1} & \textbf{Acc. Drop} & \textbf{Rel. Drop} \\
\midrule
\multirow{9}{*}{DNABERT-2} & \multirow{3}{*}{TF\_0} & 5 & 0.772 & 0.772 & 0.546 & 0.517 & 0.341 & 0.255 & 0.330 \\
& & 10 & 0.772 & 0.772 & 0.546 & 0.517 & 0.341 & 0.255 & 0.330 \\
& & 20 & 0.772 & 0.772 & 0.546 & 0.517 & 0.341 & 0.255 & 0.330 \\
\cmidrule{2-10}
& \multirow{3}{*}{TF\_1} & 5 & 0.752 & 0.752 & 0.504 & 0.496 & 0.332 & 0.256 & 0.340 \\
& & 10 & 0.752 & 0.752 & 0.504 & 0.496 & 0.332 & 0.256 & 0.340 \\
& & 20 & 0.752 & 0.752 & 0.504 & 0.496 & 0.332 & 0.256 & 0.340 \\
\cmidrule{2-10}
& \multirow{3}{*}{TF\_2} & 5 & 0.752 & 0.752 & 0.505 & 0.506 & 0.336 & 0.246 & 0.327 \\
& & 10 & 0.752 & 0.752 & 0.505 & 0.506 & 0.336 & 0.246 & 0.327 \\
& & 20 & 0.752 & 0.752 & 0.505 & 0.506 & 0.336 & 0.246 & 0.327 \\
\midrule
\multirow{9}{*}{Gene42-B} & \multirow{3}{*}{TF\_0} & 5 & 0.793 & 0.793 & 0.585 & 0.483 & 0.326 & 0.310 & 0.391 \\
& & 10 & 0.793 & 0.793 & 0.585 & 0.483 & 0.326 & 0.310 & 0.391 \\
& & 20 & 0.793 & 0.793 & 0.585 & 0.483 & 0.326 & 0.310 & 0.391 \\
\cmidrule{2-10}
& \multirow{3}{*}{TF\_1} & 5 & 0.758 & 0.758 & 0.517 & 0.504 & 0.335 & 0.254 & 0.335 \\
& & 10 & 0.758 & 0.758 & 0.517 & 0.504 & 0.335 & 0.254 & 0.335 \\
& & 20 & 0.758 & 0.758 & 0.517 & 0.504 & 0.335 & 0.254 & 0.335 \\
\cmidrule{2-10}
& \multirow{3}{*}{TF\_2} & 5 & 0.754 & 0.754 & 0.508 & 0.506 & 0.336 & 0.248 & 0.329 \\
& & 10 & 0.754 & 0.754 & 0.508 & 0.506 & 0.336 & 0.248 & 0.329 \\
& & 20 & 0.754 & 0.754 & 0.508 & 0.506 & 0.336 & 0.248 & 0.329 \\
\midrule
\multirow{9}{*}{Gene42-L} & \multirow{3}{*}{TF\_0} & 5 & 0.796 & 0.796 & 0.592 & 0.483 & 0.326 & 0.313 & 0.393 \\
& & 10 & 0.796 & 0.796 & 0.592 & 0.483 & 0.326 & 0.313 & 0.393 \\
& & 20 & 0.796 & 0.796 & 0.592 & 0.483 & 0.326 & 0.313 & 0.393 \\
\cmidrule{2-10}
& \multirow{3}{*}{TF\_1} & 5 & 0.756 & 0.756 & 0.513 & 0.504 & 0.335 & 0.252 & 0.333 \\
& & 10 & 0.756 & 0.756 & 0.513 & 0.504 & 0.335 & 0.252 & 0.333 \\
& & 20 & 0.756 & 0.756 & 0.513 & 0.504 & 0.335 & 0.252 & 0.333 \\
\cmidrule{2-10}
& \multirow{3}{*}{TF\_2} & 5 & 0.744 & 0.744 & 0.488 & 0.506 & 0.336 & 0.238 & 0.320 \\
& & 10 & 0.744 & 0.744 & 0.488 & 0.506 & 0.336 & 0.238 & 0.320 \\
& & 20 & 0.744 & 0.744 & 0.488 & 0.506 & 0.336 & 0.238 & 0.320 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{FIMBA Parameter Consistency Analysis}

A remarkable finding from our FIMBA evaluation is the complete consistency of results across different attack parameters. Table~\ref{tab:fimba_parameter_analysis} demonstrates that varying the top-k feature selection (5, 10, 20) and batch size (32, 64) produces identical results across all models and tasks.

\begin{table}[h]
\centering
\caption{FIMBA Parameter Sensitivity Analysis}
\label{tab:fimba_parameter_analysis}
\begin{tabular}{lcccc}
\toprule
\textbf{Parameter} & \textbf{Values Tested} & \textbf{Result Variation} & \textbf{Standard Deviation} & \textbf{Coefficient of Variation} \\
\midrule
Top-k features & 5, 10, 20 & None & 0.000 & 0.000 \\
Batch size & 32, 64 & None & 0.000 & 0.000 \\
Increase function & True (fixed) & N/A & N/A & N/A \\
\bottomrule
\end{tabular}
\end{table}

This parameter independence suggests several important implications:

\begin{enumerate}
    \item \textbf{Feature Importance Saturation:} The top-5 most important features contain sufficient information for effective attacks, making additional features redundant.
    
    \item \textbf{Computational Efficiency:} FIMBA attacks can be optimized to use minimal computational resources (top-5 features, batch size 32) without loss of effectiveness.
    
    \item \textbf{Biological Relevance:} The concentration of attack effectiveness in few features aligns with genomic understanding that specific positions often dominate functional importance.
\end{enumerate}

\subsection{Cross-Model Vulnerability Comparison}

Table~\ref{tab:fimba_cross_model} provides a direct comparison of FIMBA attack effectiveness across different model architectures, revealing that adversarial training provides no improvement against feature importance-based attacks.

\begin{table}[h]
\centering
\caption{FIMBA Attack Effectiveness Across Model Architectures}
\label{tab:fimba_cross_model}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Training Type} & \textbf{Avg. Clean Acc.} & \textbf{Avg. Adv. Acc.} & \textbf{Avg. Acc. Drop} & \textbf{Avg. Rel. Drop} \\
\midrule
DNABERT-2 & Standard & 0.759 & 0.509 & 0.250 & 0.332 \\
Gene42-B & Adversarial & 0.768 & 0.498 & 0.270 & 0.352 \\
Gene42-L & Adversarial & 0.764 & 0.494 & 0.270 & 0.355 \\
\midrule
\textbf{AT Improvement} & vs. DNABERT-2 & +0.006 & -0.011 & -0.020 & -0.020 \\
\bottomrule
\end{tabular}
\end{table}

The negative improvement values indicate that adversarially trained models actually perform slightly worse against FIMBA attacks, highlighting a critical limitation of current adversarial training approaches in genomic applications.

\end{document}
