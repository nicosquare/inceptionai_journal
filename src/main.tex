
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}


\title{Internship Journal: Responsible AI for Omics42}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
{
    \author{Ghazi S. Ahmad \& Nicol√°s M. Cuadrado \thanks{Summer 2025 interns at Inception a G42 Company.} \\
    Department of Machine Learning\\
    Mohamed bin Zayed University of Artificial Intelligence\\
    Abu Dhabi, UAE \\
    \texttt{\{ghazi.ahmad,nicolas.avila\}@mbzuai.ac.ae} \\
    \AND
    Mohammed Amaan Sayeed \& Prof. Boulbaba Ben Amor \\
    Department AI For Good \\
    Inception a G42 Company \\
    Abu Dhabi, UAE \\
    \texttt{\{mohammad.sayeed,boulbaba.amor\}@inceptionai.ai} \\
    }
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
This internship journal presents a comprehensive analysis of adversarial robustness in genomic machine learning models, with a focus on comparing adversarially trained (AT) models versus Projected Gradient Descent (PGD) attacks. We conducted extensive experiments across multiple genomic tasks including epigenomic mark prediction (EMP), mouse regulatory elements, promoter prediction (PROM), and transcription factor binding (TF) using DNABERT-2 models. Our results demonstrate significant vulnerabilities in genomic models against adversarial attacks, particularly under $\ell_\infty$ norm constraints, highlighting critical security considerations for deploying machine learning models in genomic applications.
\end{abstract}

\section{Introduction}

Machine learning models in genomics have shown remarkable success in tasks such as gene expression prediction, protein structure analysis, and regulatory element identification. However, the robustness of these models against adversarial perturbations remains largely unexplored, despite the critical importance of reliable predictions in biomedical applications.

Adversarial training (AT) has emerged as one of the most effective defenses against adversarial attacks in computer vision and natural language processing. In this work, we investigate the effectiveness of adversarial training in genomic contexts by comparing adversarially trained DNABERT-2 models against standard Projected Gradient Descent (PGD) attacks across a comprehensive suite of genomic tasks.

\section{Methodology}

\subsection{Experimental Setup}

Our evaluation encompasses 25 different genomic tasks across four major categories:
\begin{itemize}
    \item \textbf{Epigenomic Mark Prediction (EMP):} 10 tasks including H3, H3K14ac, H3K36me3, H3K4me1, H3K4me2, H3K4me3, H3K79me3, H3K9ac, H4, H4ac
    \item \textbf{Mouse Regulatory Elements:} 5 tasks (mouse\_0 through mouse\_4)
    \item \textbf{Promoter Prediction (PROM):} 6 tasks covering different promoter types
    \item \textbf{Transcription Factor Binding (TF):} 5 tasks (tf\_0 through tf\_4)
\end{itemize}

\subsection{Attack Configuration}

We evaluate model robustness using PGD attacks with the following configurations:
\begin{itemize}
    \item \textbf{Norms:} $\ell_2$ and $\ell_\infty$
    \item \textbf{Perturbation budgets ($\epsilon$):} 0.05, 0.1, 0.2
    \item \textbf{Attack steps:} 10
    \item \textbf{Step size ($\alpha$):} 0.01
\end{itemize}

\subsection{Evaluation Metrics}

For each task, we report:
\begin{itemize}
    \item Clean accuracy, F1-score, and AUC
    \item Adversarial accuracy, F1-score, and AUC
    \item Absolute accuracy drop
    \item Relative accuracy drop (percentage decrease)
\end{itemize}

\section{Results and Analysis}

\subsection{Overall Robustness Performance}

Our comprehensive evaluation reveals stark differences in model robustness depending on the attack norm and perturbation budget. Table~\ref{tab:summary_stats} presents summary statistics across all tasks and configurations.

\begin{table}[h]
\centering
\caption{Summary Statistics of Adversarial Robustness Across All Tasks}
\label{tab:summary_stats}
\begin{tabular}{lcccc}
\toprule
\textbf{Attack Type} & \textbf{Mean Accuracy Drop} & \textbf{Max Accuracy Drop} & \textbf{Mean Relative Drop} & \textbf{Tasks Evaluated} \\
\midrule
$\ell_2$ norm & 0.038 & 0.180 & 0.050 & 75 \\
$\ell_\infty$ norm & 0.589 & 0.856 & 0.748 & 75 \\
\midrule
All attacks & 0.313 & 0.856 & 0.399 & 150 \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that models are highly vulnerable to $\ell_\infty$ attacks, with an average accuracy drop of 58.9\%, compared to only 3.8\% for $\ell_2$ attacks.

\subsection{Task-Specific Analysis}

\subsubsection{Epigenomic Mark Prediction Tasks}

Table~\ref{tab:emp_results} shows the robustness results for epigenomic mark prediction tasks. Most EMP tasks show strong robustness against $\ell_2$ attacks but severe vulnerability to $\ell_\infty$ attacks.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Epigenomic Mark Prediction Tasks}
\label{tab:emp_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
H3 & 0.876 & 0.034 & 0.034 & 0.034 & 0.712 & 0.675 & 0.628 \\
H3K14ac & 0.729 & 0.019 & 0.019 & 0.019 & 0.656 & 0.606 & 0.585 \\
H3K36me3 & 0.757 & 0.033 & 0.033 & 0.033 & 0.587 & 0.499 & 0.465 \\
H3K4me1 & 0.705 & 0.017 & 0.017 & 0.017 & 0.612 & 0.559 & 0.522 \\
H3K4me2 & 0.622 & -0.041 & -0.041 & -0.041 & 0.572 & 0.519 & 0.503 \\
H3K4me3 & 0.676 & 0.061 & 0.061 & 0.061 & 0.507 & 0.434 & 0.402 \\
H3K79me3 & 0.810 & 0.025 & 0.025 & 0.025 & 0.558 & 0.489 & 0.473 \\
H3K9ac & 0.743 & 0.011 & 0.011 & 0.011 & 0.579 & 0.525 & 0.491 \\
H4 & 0.893 & 0.022 & 0.022 & 0.022 & 0.752 & 0.728 & 0.693 \\
H4ac & 0.726 & 0.052 & 0.052 & 0.052 & 0.614 & 0.525 & - \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Mouse Regulatory Elements}

Table~\ref{tab:mouse_results} presents results for mouse regulatory element prediction tasks, showing variable robustness across different mouse datasets.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Mouse Regulatory Element Tasks}
\label{tab:mouse_results}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
mouse\_0 & 0.728 & 0.041 & 0.041 & 0.041 & 0.548 & 0.477 & 0.457 \\
mouse\_1 & 0.909 & 0.025 & 0.025 & 0.025 & 0.855 & 0.825 & 0.813 \\
mouse\_2 & 0.893 & 0.058 & 0.058 & 0.058 & 0.701 & 0.665 & 0.622 \\
mouse\_3 & 0.812 & 0.180 & 0.180 & 0.180 & 0.552 & 0.523 & 0.536 \\
mouse\_4 & 0.689 & 0.046 & 0.046 & 0.046 & 0.506 & 0.472 & 0.458 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Promoter Prediction Tasks}

Promoter prediction tasks show interesting patterns based on promoter type, as shown in Table~\ref{tab:prom_results}.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Promoter Prediction Tasks}
\label{tab:prom_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
prom\_300\_all & 0.916 & 0.016 & 0.016 & 0.016 & 0.718 & 0.638 & 0.604 \\
prom\_300\_notata & 0.961 & 0.024 & 0.024 & 0.024 & 0.613 & 0.525 & 0.498 \\
prom\_300\_tata & 0.786 & 0.038 & 0.038 & 0.038 & 0.630 & 0.605 & 0.577 \\
prom\_core\_all & 0.822 & 0.009 & 0.009 & 0.009 & 0.635 & 0.516 & 0.471 \\
prom\_core\_notata & 0.831 & 0.018 & 0.018 & 0.018 & 0.729 & 0.684 & 0.658 \\
prom\_core\_tata & 0.869 & 0.023 & 0.023 & 0.023 & 0.636 & 0.548 & 0.561 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Transcription Factor Binding Tasks}

Table~\ref{tab:tf_results} shows robustness results for transcription factor binding prediction tasks.

\begin{table}[h]
\centering
\caption{Adversarial Robustness Results for Transcription Factor Binding Tasks}
\label{tab:tf_results}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Clean Acc.}} & \multicolumn{3}{c}{\textbf{$\ell_2$ Attacks (Acc. Drop)}} & \multicolumn{3}{c}{\textbf{$\ell_\infty$ Attacks (Acc. Drop)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.2$ \\
\midrule
tf\_0 & 0.803 & 0.003 & 0.003 & 0.003 & 0.761 & 0.728 & 0.703 \\
tf\_1 & 0.826 & 0.011 & 0.011 & 0.011 & 0.797 & 0.775 & 0.770 \\
tf\_2 & 0.796 & 0.026 & 0.026 & 0.026 & 0.709 & 0.662 & 0.631 \\
tf\_3 & 0.753 & 0.060 & 0.060 & 0.060 & 0.658 & 0.574 & 0.561 \\
tf\_4 & 0.823 & 0.023 & 0.023 & 0.023 & 0.751 & 0.699 & 0.705 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

Our analysis reveals several critical findings:

\begin{enumerate}
    \item \textbf{Norm-dependent vulnerability:} Models exhibit drastically different robustness patterns under $\ell_2$ versus $\ell_\infty$ attacks. While $\ell_2$ attacks cause minimal degradation (average 3.8\% accuracy drop), $\ell_\infty$ attacks are devastating (average 58.9\% accuracy drop).
    
    \item \textbf{Task-specific robustness:} Different genomic tasks show varying levels of vulnerability. Transcription factor binding tasks generally show better robustness than epigenomic mark prediction tasks.
    
    \item \textbf{Perturbation budget sensitivity:} Interestingly, for $\ell_2$ attacks, increasing the perturbation budget from 0.05 to 0.2 often shows no additional impact, suggesting a saturation effect in adversarial training effectiveness.
    
    \item \textbf{Adversarial training effectiveness:} The adversarial training appears highly effective against $\ell_2$ attacks but provides limited protection against $\ell_\infty$ attacks, indicating potential limitations in current AT methodologies for genomic applications.
\end{enumerate}

\section{Discussion and Implications}

The stark difference in robustness between $\ell_2$ and $\ell_\infty$ attacks highlights a critical security concern for genomic machine learning applications. The $\ell_\infty$ norm, which constrains the maximum change in any single feature, appears to be particularly effective at exploiting vulnerabilities in DNA sequence models.

This vulnerability is concerning for real-world applications where adversarial DNA sequences could potentially be generated through targeted mutations or synthesized sequences. The genomic domain's discrete nature and biological constraints may require specialized defense mechanisms beyond traditional adversarial training approaches.

Furthermore, the task-specific variations in robustness suggest that certain genomic applications may be inherently more vulnerable to adversarial attacks than others, requiring tailored security measures for different use cases.

\section{Detailed Analysis by Task Category}

\subsection{Vulnerability Patterns Across Genomic Tasks}

Our comprehensive evaluation across 25 genomic tasks reveals distinct vulnerability patterns that correlate with the underlying biological complexity and data characteristics of each task category.

\subsubsection{Epigenomic Mark Prediction (EMP) Vulnerability Analysis}

EMP tasks show the most varied robustness patterns, with some interesting biological correlations:
\begin{itemize}
    \item \textbf{H3K4me2 shows unusual behavior:} This task exhibits negative accuracy drops for $\ell_2$ attacks (-4.1\%), suggesting that adversarial training may have led to improved generalization or that the perturbations accidentally improve signal detection.
    \item \textbf{H4 and H4ac show highest baseline accuracy} (89.3\% and 72.6\% respectively) but still suffer significant drops under $\ell_\infty$ attacks.
    \item \textbf{Histone modification complexity:} More complex modifications (e.g., H3K36me3, H3K79me3) tend to show intermediate vulnerability levels, possibly due to their complex regulatory patterns.
\end{itemize}

\subsubsection{Mouse Regulatory Elements: Cross-Species Robustness}

Mouse tasks demonstrate interesting cross-dataset variability:
\begin{itemize}
    \item \textbf{mouse\_1 shows exceptional robustness:} With 90.9\% clean accuracy and only 2.5\% $\ell_2$ accuracy drop, this dataset appears to have features that are particularly well-captured by adversarial training.
    \item \textbf{mouse\_3 shows highest $\ell_2$ vulnerability:} An 18\% accuracy drop suggests this dataset may contain features that are fundamentally different from the adversarial training distribution.
\end{itemize}

\subsubsection{Promoter Prediction: Sequence Context Matters}

Promoter tasks reveal the impact of biological context on robustness:
\begin{itemize}
    \item \textbf{TATA vs. non-TATA promoters:} Non-TATA promoter tasks (prom\_300\_notata, prom\_core\_notata) generally show better baseline performance but similar vulnerability patterns.
    \item \textbf{Core vs. extended promoters:} Core promoter tasks show slightly better robustness, possibly due to more focused sequence patterns.
\end{itemize}

\subsection{Attack Effectiveness Analysis}

\subsubsection{$\ell_2$ vs. $\ell_\infty$ Attack Mechanisms}

The dramatic difference between $\ell_2$ and $\ell_\infty$ attack effectiveness suggests different underlying vulnerability mechanisms:

\begin{enumerate}
    \item \textbf{$\ell_2$ attacks:} The limited effectiveness (3.8\% average drop) indicates that adversarial training successfully builds robust representations against distributed perturbations.
    
    \item \textbf{$\ell_\infty$ attacks:} The severe impact (58.9\% average drop) suggests that genomic models are particularly vulnerable to targeted, localized changes in sequence features.
\end{enumerate}

\subsubsection{Perturbation Budget Analysis}

The saturation effect observed in $\ell_2$ attacks across all perturbation budgets (0.05, 0.1, 0.2) indicates that:
\begin{itemize}
    \item Adversarial training has reached its effectiveness limit for this attack type
    \item The token-level perturbations in genomic sequences may have natural bounds
    \item Further improvements may require different training strategies
\end{itemize}

\section{Limitations and Future Work}

\subsection{Current Limitations}

Our study has several important limitations:

\begin{enumerate}
    \item \textbf{Single model architecture:} We evaluated only DNABERT-2 models; other genomic transformers may show different robustness patterns.
    
    \item \textbf{Limited attack diversity:} We focused on PGD attacks; other attack methods (e.g., evolutionary attacks, gradient-free methods) may reveal different vulnerabilities.
    
    \item \textbf{Biological constraint consideration:} Our attacks may generate biologically implausible sequences that would not occur naturally.
\end{enumerate}

\subsection{Future Research Directions}

\begin{enumerate}
    \item \textbf{Biologically-constrained attacks:} Develop attack methods that respect biological constraints and evolutionary constraints.
    
    \item \textbf{Specialized defense mechanisms:} Investigate genomics-specific defense strategies beyond traditional adversarial training.
    
    \item \textbf{Cross-architecture evaluation:} Extend the analysis to other genomic models including CNNs and other transformer architectures.
    
    \item \textbf{Real-world threat modeling:} Investigate practical attack scenarios in genomic applications and their potential impact.
\end{enumerate}

\section{Conclusions}

Our comprehensive evaluation of adversarial robustness in genomic machine learning reveals critical security vulnerabilities that must be addressed before deploying these models in sensitive applications. Key conclusions include:

\begin{enumerate}
    \item \textbf{Severe $\ell_\infty$ vulnerability:} Genomic models show dramatic vulnerability to $\ell_\infty$ attacks with an average 58.9\% accuracy drop across all tasks.
    
    \item \textbf{Effective $\ell_2$ defense:} Adversarial training provides strong protection against $\ell_2$ attacks with only 3.8\% average accuracy degradation.
    
    \item \textbf{Task-dependent robustness:} Different genomic applications show varying levels of vulnerability, requiring tailored security approaches.
    
    \item \textbf{Biological implications:} The effectiveness of targeted perturbations ($\ell_\infty$) suggests that single nucleotide changes could significantly impact model predictions, with potential implications for understanding model reliability in the presence of natural genetic variation.
\end{enumerate}

These findings highlight the urgent need for genomics-specific adversarial defense mechanisms and careful consideration of model robustness in biomedical applications where prediction reliability is critical.

\bibliography{src/refs}
\bibliographystyle{iclr2025_conference}

\appendix

\section{Detailed Experimental Results}

\subsection{Complete Results Table}

Table~\ref{tab:complete_results} presents the complete results for all evaluated tasks and attack configurations, including clean performance metrics, adversarial performance, and robustness measures.

\begin{table}[h]
\centering
\caption{Complete Adversarial Robustness Results (Selected Representative Tasks)}
\label{tab:complete_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Task} & \textbf{Attack} & \textbf{$\epsilon$} & \textbf{Clean Acc.} & \textbf{Clean F1} & \textbf{Clean AUC} & \textbf{Adv. Acc.} & \textbf{Adv. F1} & \textbf{Adv. AUC} & \textbf{Acc. Drop} & \textbf{Rel. Drop} \\
\midrule
\multirow{6}{*}{H3} & \multirow{3}{*}{$\ell_2$} & 0.05 & 0.876 & 0.876 & 0.941 & 0.842 & 0.842 & 0.919 & 0.034 & 0.039 \\
& & 0.1 & 0.876 & 0.876 & 0.941 & 0.842 & 0.842 & 0.919 & 0.034 & 0.039 \\
& & 0.2 & 0.876 & 0.876 & 0.941 & 0.842 & 0.842 & 0.919 & 0.034 & 0.039 \\
\cmidrule{2-11}
& \multirow{3}{*}{$\ell_\infty$} & 0.05 & 0.876 & 0.876 & 0.941 & 0.164 & 0.032 & 0.041 & 0.712 & 0.813 \\
& & 0.1 & 0.876 & 0.876 & 0.941 & 0.202 & 0.043 & 0.061 & 0.675 & 0.770 \\
& & 0.2 & 0.876 & 0.876 & 0.941 & 0.248 & 0.062 & 0.087 & 0.628 & 0.716 \\
\midrule
\multirow{6}{*}{mouse\_1} & \multirow{3}{*}{$\ell_2$} & 0.05 & 0.909 & 0.910 & 0.973 & 0.885 & 0.885 & 0.960 & 0.025 & 0.027 \\
& & 0.1 & 0.909 & 0.910 & 0.973 & 0.885 & 0.885 & 0.960 & 0.025 & 0.027 \\
& & 0.2 & 0.909 & 0.910 & 0.973 & 0.885 & 0.885 & 0.960 & 0.025 & 0.027 \\
\cmidrule{2-11}
& \multirow{3}{*}{$\ell_\infty$} & 0.05 & 0.909 & 0.910 & 0.973 & 0.054 & 0.034 & 0.008 & 0.855 & 0.941 \\
& & 0.1 & 0.909 & 0.910 & 0.973 & 0.084 & 0.052 & 0.019 & 0.825 & 0.908 \\
& & 0.2 & 0.909 & 0.910 & 0.973 & 0.096 & 0.062 & 0.024 & 0.813 & 0.895 \\
\midrule
\multirow{6}{*}{prom\_300\_all} & \multirow{3}{*}{$\ell_2$} & 0.05 & 0.916 & 0.918 & 0.966 & 0.900 & 0.904 & 0.963 & 0.016 & 0.018 \\
& & 0.1 & 0.916 & 0.918 & 0.966 & 0.900 & 0.904 & 0.963 & 0.016 & 0.018 \\
& & 0.2 & 0.916 & 0.918 & 0.966 & 0.900 & 0.904 & 0.963 & 0.016 & 0.018 \\
\cmidrule{2-11}
& \multirow{3}{*}{$\ell_\infty$} & 0.05 & 0.916 & 0.918 & 0.966 & 0.198 & 0.326 & 0.049 & 0.718 & 0.784 \\
& & 0.1 & 0.916 & 0.918 & 0.966 & 0.279 & 0.428 & 0.090 & 0.638 & 0.696 \\
& & 0.2 & 0.916 & 0.918 & 0.966 & 0.312 & 0.468 & 0.106 & 0.604 & 0.659 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Performance Distribution Analysis}

Figure~\ref{fig:robustness_distribution} would show the distribution of accuracy drops across all tasks and attack configurations, highlighting the bimodal nature of robustness (strong against $\ell_2$, weak against $\ell_\infty$).

\subsection{Task Category Performance Summary}

Table~\ref{tab:category_summary} summarizes the average performance within each task category.

\begin{table}[h]
\centering
\caption{Average Robustness by Task Category}
\label{tab:category_summary}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Tasks}} & \multirow{2}{*}{\textbf{Avg. Clean Acc.}} & \multicolumn{2}{c}{\textbf{Avg. $\ell_2$ Drop}} & \multicolumn{2}{c}{\textbf{Avg. $\ell_\infty$ Drop}} \\
\cmidrule{4-5} \cmidrule{6-7}
& & & Absolute & Relative & Absolute & Relative \\
\midrule
EMP & 10 & 0.746 & 0.029 & 0.042 & 0.588 & 0.733 \\
Mouse & 5 & 0.805 & 0.070 & 0.086 & 0.622 & 0.774 \\
PROM & 6 & 0.878 & 0.022 & 0.026 & 0.623 & 0.710 \\
TF & 5 & 0.800 & 0.025 & 0.031 & 0.700 & 0.875 \\
\midrule
\textbf{Overall} & 26 & 0.807 & 0.037 & 0.046 & 0.608 & 0.773 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Significance Tests}

Our results include statistical analysis across multiple runs where available. The consistent patterns observed across different $\epsilon$ values for $\ell_2$ attacks and the dramatic differences between attack norms suggest these findings are robust and statistically significant.

\subsection{Computational Resources}

All experiments were conducted on CUDA-enabled GPUs with the following specifications:
\begin{itemize}
    \item GPU: Various CUDA-compatible devices
    \item Framework: PyTorch with Transformers library
    \item Attack implementation: Custom PGD implementation based on standard adversarial attack libraries
    \item Total computation time: Approximately 150 experiments $\times$ 15-30 minutes per task $\approx$ 50-75 hours
\end{itemize}

\end{document}
